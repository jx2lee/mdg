---
{"dg-publish":true,"permalink":"/kakaopay-20241208/","hide":true,"tags":[null],"dgLinkPreview":true,"noteIcon":"","created":"2024-12-07T14:51:46.588+09:00"}
---


### Debezium 을 이용한 CDC 파이프라인 구축
1. 해당 프로젝트에서 다루었던 데이터:
    - Debezium을 통해 수집된 Change Data Capture (CDC) 데이터로, create/update/delete 이벤트가 발생하는 테이블의 변경 데이터
    - 수집된 CDC 데이터를 저장하는 Kafka 토픽
    - BigQuery에 최종적으로 적재되는 CDC 데이터
    - Debezium 커넥터의 성능 및 상태를 모니터링하기 위한 JMX 메트릭
2. 프로젝트 수행 시 참여했던 인원:
    - 데이터 엔지니어 (프로젝트 주도 및 구현)
    - DBA (데이터베이스 관련 협의 및 지원)
    - 인프라 엔지니어 (AWS EKS, MSK 등 인프라 지원)
3. 프로젝트 수행 시 자신의 비중:
    - 프로젝트 설계 및 주도
        - Debezium을 이용한 CDC 파이프라인 구축
        - AWS EKS에 Kafka Connect 클러스터 배포
        - Debezium 소스 커넥터 및 BigQuery 싱크 커넥터 구성
        - 모니터링 시스템 구축 (Prometheus, Grafana)
    - DBA와의 커뮤니케이션 및 최적화 작업
    - 트러블슈팅 (커넥터 재실행 문제 해결)
    - CDC 데이터 활용 방안 제시 (dbt [lambda view](https://github.com/dbt-labs/dbt-labs-experimental-features/tree/main/lambda-views))
4. 더보기: [[data/kafka/__/cdc-pipeline-with-debezium\|Debezium 을 이용한 CDC 파이프라인 구축]]


### 서버로그 파이프라인 마이그레이션 (Kinesis to Kafka)
1. 해당 프로젝트에서 다루었던 데이터:
    - 거래소 회원 서비스의 이벤트 데이터
    - AWS Kinesis에서 발행되던 메시지
    - BigQuery에 적재되는 서버 로그 데이터
    - Kafka Connect 클러스터의 성능 및 상태 모니터링 메트릭
2. 프로젝트 수행 시 참여했던 인원:  
    - 데이터 엔지니어 (프로젝트 주도 및 구현)
    - 백엔드 개발자 (EDA 아키텍처 전환 작업 관련)
    - 인프라 엔지니어 (AWS MSK, EKS 등 인프라 지원)
3. 프로젝트 수행 시 자신의 비중:  
    - Kafka Connect를 이용한 새로운 파이프라인 구축
    - AWS EKS에 Kafka Connect 클러스터 배포
        - 헬름 차트를 이용한 Kafka Connect 및 Connect UI 배포 관리
    - BigQuery Sink Connector 구성 및 관리
    - 모니터링 시스템 구축 (Grafana 대시보드 및 알림 설정)
    - 재처리 로직 설계
4. 더보기: [[data/kafka/__/migration-to-kakfa-connect\|서버로그 파이프라인 마이그레이션]]


### Github Action 을 활용한 dbt CI/CD 파이프라인 구축
1. 해당 프로젝트에서 다루었던 데이터:
    - BigQuery 에 저장되는 변환된 데이터
    - GitHub 이벤트 데이터 (push, PR 등)
2. 프로젝트 수행 시 참여했던 인원:  
    - 데이터 엔지니어 (프로젝트 주도 및 구현)
3. 프로젝트 수행 시 자신의 비중:  
    - 프로젝트 설계 및 주도
        - GitHub Actions 워크플로우 구성
        - dbt CI/CD 파이프라인 구축
        - 커스텀 액션 개발 및 관리
        - Reviewdog, Jira 통합 등 추가 기능 구현
    - 사용자 경험 개선을 위한 지속적인 기능 추가
    - 기존 파이프라인에서 새로운 파이프라인으로의 이관 작업
4. 더보기: [[data/dbt/__/dbt-cicd-pipeline\|Github Action 을 활용한 dbt CI/CD 파이프라인 구축]]


### Self Serve Data
1. 해당 프로젝트에서 다루었던 데이터:
    - 내부 RDB (exchange, account, wallet) 데이터
    - BigQuery에 저장된 데이터
    - 외부 데이터 (외부 거래소 데이터, API 데이터 등)
    - 데이터 카탈로그 및 메타데이터(dbt 모델 메타데이터)
    - 사용자 생성 워크플로우 및 파이프라인 데이터
    - 데이터 품질 및 리니지 정보
2. 프로젝트 수행 시 참여했던 인원:
    - 데이터 엔지니어 (프로젝트 주도 및 구현)
    - 인프라 엔지니어 (Kubernetes, ArgoCD 관리)
    - 보안 팀 멤버 (DataHub 활용)
    - 데이터 추출 멤버 (MageAI 활용)
3. 프로젝트 수행 시 자신의 비중:
    - 프로젝트 설계 및 주도
        - DataHub, Airbyte, MageAI 도구 선정 및 구축
        - Kubernetes 환경에 도구들 배포 및 관리
        - 커스텀 Airbyte 커넥터 개발
        - DataHub 메타데이터 수집 스크립트 작성 및 Airflow 파이프라인 구성
    - MageAI 환경 커스터마이징 및 오픈소스 기여
    - 사용자 교육 및 지원
    - 문서화
4. 더보기: [[data/etc/self-serve-data\|self-serve-data]]


### 그 외 정기업무

배치 파이프라인 개발 & 유지보수
1. 해당 프로젝트에서 다루었던 데이터:
2. 프로젝트 수행 시 참여했던 인원:
3. 프로젝트 수행 시 자신의 비중:


GCP(Google Cloud Platform) 운영
1. 해당 프로젝트에서 다루었던 데이터:
2. 프로젝트 수행 시 참여했던 인원:
3. 프로젝트 수행 시 자신의 비중: